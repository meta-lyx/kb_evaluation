{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FiQA dataset...\n",
      "\n",
      "Creating dataset: FiQA_Evaluation_Dataset\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "409 Client Error: CONFLICT for url: https://api.dify.ai/v1/datasets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFiQA_Evaluation_Dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreating dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m dataset_id \u001b[38;5;241m=\u001b[39m dify_manager\u001b[38;5;241m.\u001b[39mcreate_dataset(dataset_name)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated dataset with ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# 上传前20个文档\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[35], line 25\u001b[0m, in \u001b[0;36mDifyDatasetManager.create_dataset\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     19\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: name,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpermission\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_me\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m }\n\u001b[1;32m     24\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[0;32m---> 25\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 409 Client Error: CONFLICT for url: https://api.dify.ai/v1/datasets"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "\n",
    "class DifyDatasetManager:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.dify.ai/v1\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def create_dataset(self, name: str) -> str:\n",
    "        \"\"\"创建新的 Dify 数据集\"\"\"\n",
    "        url = f\"{self.base_url}/datasets\"\n",
    "        payload = {\n",
    "            \"name\": name,\n",
    "            \"permission\": \"only_me\"\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, headers=self.headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json()['id']\n",
    "    \n",
    "    def create_document(self, dataset_id: str, title: str, content: str) -> bool:\n",
    "        \"\"\"通过文本创建文档\"\"\"\n",
    "        url = f\"{self.base_url}/datasets/{dataset_id}/document/create-by-text\"\n",
    "        \n",
    "        payload = {\n",
    "            \"name\": title,\n",
    "            \"text\": content,\n",
    "            \"indexing_technique\": \"high_quality\",\n",
    "            \"process_rule\": {\"mode\": \"automatic\"}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=self.headers, json=payload)\n",
    "            if response.status_code == 200:\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Error creating document: {response.text}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"Exception during document creation: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def retrieve(self, dataset_id: str, query: str, top_k: int = 1) -> list:\n",
    "        \"\"\"从数据集中检索文档\"\"\"\n",
    "        url = f\"{self.base_url}/datasets/{dataset_id}/retrieve\"\n",
    "        \n",
    "        payload = {\n",
    "            \"query\": query,\n",
    "            \"retrieval_model\": {\n",
    "                \"search_method\": \"semantic_search\",\n",
    "                \"reranking_enable\": False,\n",
    "                \"top_k\": top_k\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, headers=self.headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get('documents', [])\n",
    "\n",
    "# 初始化\n",
    "DIFY_API_KEY = \"dataset-vGo3aB5ogicKiIsdWQ5FmBE2\"\n",
    "dify_manager = DifyDatasetManager(DIFY_API_KEY)\n",
    "\n",
    "# 加载数据集\n",
    "print(\"Loading FiQA dataset...\")\n",
    "fiqa_corpus = load_dataset(\"BeIR/fiqa\", \"corpus\")\n",
    "fiqa_queries = load_dataset(\"BeIR/fiqa\", \"queries\")\n",
    "fiqa_qrels = load_dataset(\"BeIR/fiqa-qrels\")\n",
    "\n",
    "# 创建新的数据集\n",
    "dataset_name = \"FiQA_Evaluation_Dataset\"\n",
    "print(f\"\\nCreating dataset: {dataset_name}\")\n",
    "dataset_id = dify_manager.create_dataset(dataset_name)\n",
    "print(f\"Created dataset with ID: {dataset_id}\")\n",
    "\n",
    "# 上传前20个文档\n",
    "print(\"\\nUploading first 20 documents...\")\n",
    "for idx, doc in enumerate(tqdm(corpus_data[:20])):\n",
    "    title = f\"Document_{idx+1}\"\n",
    "    success = dify_manager.create_document(\n",
    "        dataset_id=dataset_id,\n",
    "        title=title,\n",
    "        content=doc['text']\n",
    "    )\n",
    "    time.sleep(1)  # Rate limiting\n",
    "\n",
    "# 使用前20个查询评估检索效果\n",
    "print(\"\\nEvaluating retrieval performance...\")\n",
    "results = []\n",
    "\n",
    "for idx, query in enumerate(tqdm(queries_data[:20])):\n",
    "    query_text = query['text']\n",
    "    \n",
    "    # 获取检索结果\n",
    "    retrieved_docs = dify_manager.retrieve(\n",
    "        dataset_id=dataset_id,\n",
    "        query=query_text,\n",
    "        top_k=3\n",
    "    )\n",
    "    \n",
    "    results.append({\n",
    "        'query_id': idx + 1,\n",
    "        'query': query_text,\n",
    "        'retrieved_docs': retrieved_docs\n",
    "    })\n",
    "    time.sleep(1)  # Rate limiting\n",
    "\n",
    "# 保存评估结果\n",
    "print(\"\\nSaving results...\")\n",
    "with open('dify_evaluation_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# 打印示例结果\n",
    "print(\"\\nSample Results:\")\n",
    "for result in results[:2]:\n",
    "    print(f\"\\nQuery: {result['query']}\")\n",
    "    print(\"Retrieved documents:\")\n",
    "    for idx, doc in enumerate(result['retrieved_docs'], 1):\n",
    "        print(f\"{idx}. Score: {doc.get('score', 'N/A')}\")\n",
    "        print(f\"   Text: {doc.get('text', '')[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since BeIR/fiqa couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'corpus' at /Users/yingxuanli/.cache/huggingface/datasets/BeIR___fiqa/corpus/0.0.0/e31855201132ea2a257d7df77c828d7c02427521 (last modified on Sun Mar  9 16:03:04 2025).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892dd9355bee45cbacdca0d7e3cad710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/14.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21e7f8b2abc4f0fa45e01476116ecfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.tsv:   0%|          | 0.00/210k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32551d5fd72e473fb1f4a633cc606a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev.tsv:   0%|          | 0.00/18.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "322c4a6ddc4442989c9cb77a9e2c8b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.tsv:   0%|          | 0.00/25.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a024507dfd45228c48eebf2e75b62a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69774d3c60ba46f887e308076e4dfc14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1238 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a033423384444bb91e3059d91fedb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1706 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "fiqa_corpus = load_dataset(\"BeIR/fiqa\", \"corpus\")\n",
    "fiqa_queries = load_dataset(\"BeIR/fiqa\", \"queries\")\n",
    "fiqa_qrels = load_dataset(\"BeIR/fiqa-qrels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FiQA dataset...\n",
      "\n",
      "Dataset structure:\n",
      "Split 'corpus' contains 57638 documents\n",
      "Features: ['_id', 'title', 'text']\n",
      "\n",
      "Creating dataset: FiQA_Financial_Dataset\n",
      "Failed to create dataset: 'DifyDatasetManager' object has no attribute 'create_dataset'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DifyDatasetManager' object has no attribute 'create_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCreating dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     dataset_id \u001b[38;5;241m=\u001b[39m dify_manager\u001b[38;5;241m.\u001b[39mcreate_dataset(dataset_name)\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated dataset with ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DifyDatasetManager' object has no attribute 'create_dataset'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class DifyDatasetManager:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://api.dify.ai/v1\"\n",
    "        # Add Content-Type header\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    \n",
    "    def upload_document(self, dataset_id: str, title: str, content: str) -> bool:\n",
    "        \"\"\"上传单个文档到数据集\"\"\"\n",
    "        # 使用正确的API端点\n",
    "        url = f\"{self.base_url}/datasets/{dataset_id}/documents\"\n",
    "        \n",
    "        payload = {\n",
    "            \"name\": title,\n",
    "            \"text\": content\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(url, headers=self.headers, json=payload)\n",
    "            if response.status_code == 200:\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Error uploading document: {response.text}\")\n",
    "                print(f\"Status code: {response.status_code}\")\n",
    "                return False\n",
    "        except Exception as e:\n",
    "            print(f\"Exception during upload: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "def prepare_documents_file(corpus_data):\n",
    "    \"\"\"准备文档文件\"\"\"\n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as f:\n",
    "        for doc in corpus_data:\n",
    "            title = doc.get('title', '')\n",
    "            text = doc.get('text', '')\n",
    "            f.write(f\"# {title}\\n{text}\\n###\\n\")\n",
    "        return f.name\n",
    "\n",
    "# 初始化\n",
    "DIFY_API_KEY = \"dataset-vGo3aB5ogicKiIsdWQ5FmBE2\"\n",
    "dify_manager = DifyDatasetManager(DIFY_API_KEY)\n",
    "\n",
    "# 加载 FiQA 数据集\n",
    "print(\"Loading FiQA dataset...\")\n",
    "fiqa_corpus = load_dataset(\"BeIR/fiqa\", \"corpus\")\n",
    "\n",
    "# 检查数据集结构\n",
    "print(\"\\nDataset structure:\")\n",
    "for split, dataset in fiqa_corpus.items():\n",
    "    print(f\"Split '{split}' contains {len(dataset)} documents\")\n",
    "    print(f\"Features: {list(dataset.features.keys())}\")\n",
    "\n",
    "# 创建新的数据集\n",
    "dataset_name = \"FiQA_Financial_Dataset\"\n",
    "print(f\"\\nCreating dataset: {dataset_name}\")\n",
    "try:\n",
    "    dataset_id = dify_manager.create_dataset(dataset_name)\n",
    "    print(f\"Created dataset with ID: {dataset_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to create dataset: {e}\")\n",
    "    raise\n",
    "\n",
    "# 上传所有文档\n",
    "print(\"\\nUploading documents...\")\n",
    "success_count = 0\n",
    "total_docs = sum(len(dataset) for dataset in fiqa_corpus.values())\n",
    "\n",
    "# 遍历所有分片中的文档\n",
    "for split, dataset in fiqa_corpus.items():\n",
    "    print(f\"\\nProcessing split: {split}\")\n",
    "    \n",
    "    for idx, doc in enumerate(tqdm(dataset)):\n",
    "        if idx % 10 == 0:  # Add delay every 10 requests to avoid rate limits\n",
    "            time.sleep(1)\n",
    "        \n",
    "        # 安全地获取文档字段\n",
    "        doc_id = doc['_id']\n",
    "        doc_title = doc.get('title', '')  # 使用 get() 方法提供默认值\n",
    "        doc_text = doc.get('text', '')\n",
    "        \n",
    "        title = f\"Document_{split}_{doc_id}\"\n",
    "        \n",
    "        success = dify_manager.upload_document(\n",
    "            dataset_id=dataset_id,\n",
    "            title=title,\n",
    "            content=doc_text\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            success_count += 1\n",
    "            if success_count % 50 == 0:\n",
    "                print(f\"\\nProgress: {success_count}/{total_docs} documents uploaded\")\n",
    "\n",
    "# 打印最终结果\n",
    "print(\"\\nUpload Summary:\")\n",
    "print(f\"Dataset ID: {dataset_id}\")\n",
    "print(f\"Total documents: {total_docs}\")\n",
    "print(f\"Successfully uploaded: {success_count}\")\n",
    "print(f\"Failed uploads: {total_docs - success_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DifyRetriever:\n",
    "    def __init__(self, api_key: str, dataset_id: str):\n",
    "        self.api_key = api_key\n",
    "        self.dataset_id = dataset_id\n",
    "        self.base_url = \"https://api.dify.ai/v1\"\n",
    "        \n",
    "    def retrieve(self, query: str, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"从 Dify 知识库检索文档\"\"\"\n",
    "        url = f\"{self.base_url}/datasets/{self.dataset_id}/retrieve\"\n",
    "        \n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"query\": query,\n",
    "            \"retrieval_model\": {\n",
    "                \"search_method\": \"semantic_search\",\n",
    "                \"reranking_enable\": True,\n",
    "                \"top_k\": top_k\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        return response.json().get(\"data\", [])\n",
    "\n",
    "# 加载 FiQA 数据集的查询\n",
    "from datasets import load_dataset\n",
    "fiqa_queries = load_dataset(\"BeIR/fiqa\", \"queries\")\n",
    "\n",
    "# 转换为更易处理的格式\n",
    "queries = {\n",
    "    row['_id']: row['text'] \n",
    "    for row in fiqa_queries['train']\n",
    "}\n",
    "\n",
    "# Dify API 配置\n",
    "DIFY_API_KEY = \"your_api_key_here\"\n",
    "DATASET_ID = \"your_dataset_id_here\"\n",
    "retriever = DifyRetriever(DIFY_API_KEY, DATASET_ID)\n",
    "\n",
    "# 执行检索评估\n",
    "def evaluate_retrieval(queries: Dict[str, str], top_k: int = 10):\n",
    "    results = {}\n",
    "    for query_id, query_text in tqdm(queries.items(), desc=\"Evaluating queries\"):\n",
    "        try:\n",
    "            retrieved_docs = retriever.retrieve(query_text, top_k=top_k)\n",
    "            results[query_id] = {\n",
    "                'query': query_text,\n",
    "                'retrieved': retrieved_docs\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing query {query_id}: {e}\")\n",
    "    return results\n",
    "\n",
    "# 运行评估\n",
    "print(\"开始评估...\")\n",
    "evaluation_results = evaluate_retrieval(queries, top_k=10)\n",
    "\n",
    "# 显示评估结果\n",
    "print(\"\\n评估统计:\")\n",
    "print(f\"总查询数: {len(queries)}\")\n",
    "print(f\"成功检索数: {len(evaluation_results)}\")\n",
    "\n",
    "# 显示样例结果\n",
    "sample_query_id = list(evaluation_results.keys())[0]\n",
    "print(\"\\n样例查询结果:\")\n",
    "print(f\"Query: {queries[sample_query_id]}\")\n",
    "print(\"Top retrieved documents:\")\n",
    "for i, doc in enumerate(evaluation_results[sample_query_id]['retrieved'][:3], 1):\n",
    "    print(f\"\\n{i}. Score: {doc.get('score', 'N/A')}\")\n",
    "    print(f\"Text: {doc.get('text', '')[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert corpus to dictionary format\u001b[39;00m\n\u001b[1;32m      2\u001b[0m corpus \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (doc_id, title, text) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\n\u001b[0;32m----> 4\u001b[0m     fiqa_corpus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m'\u001b[39m],  \u001b[38;5;66;03m# Using 'test' instead of 'train'\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     fiqa_corpus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m     fiqa_corpus[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      7\u001b[0m )):\n\u001b[1;32m      8\u001b[0m     corpus[doc_id] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: title,\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text\n\u001b[1;32m     11\u001b[0m     }\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Convert queries to dictionary format\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/datasets/dataset_dict.py:72\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(k)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     74\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     75\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m     76\u001b[0m         ]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert corpus to dictionary format\n",
    "corpus = {}\n",
    "for idx, (doc_id, title, text) in enumerate(zip(\n",
    "    fiqa_corpus['test']['_id'],  # Using 'test' instead of 'train'\n",
    "    fiqa_corpus['test']['title'],\n",
    "    fiqa_corpus['test']['text']\n",
    ")):\n",
    "    corpus[doc_id] = {\n",
    "        \"title\": title,\n",
    "        \"text\": text\n",
    "    }\n",
    "\n",
    "# Convert queries to dictionary format\n",
    "queries = {}\n",
    "for query_id, query_text in zip(\n",
    "    fiqa_queries['test']['_id'],\n",
    "    fiqa_queries['test']['text']\n",
    "):\n",
    "    queries[query_id] = query_text\n",
    "\n",
    "# Print sample data\n",
    "print(\"Sample Document:\")\n",
    "sample_doc_id = list(corpus.keys())[0]\n",
    "print(f\"\\nDocument ID: {sample_doc_id}\")\n",
    "print(f\"Title: {corpus[sample_doc_id]['title']}\")\n",
    "print(f\"Text: {corpus[sample_doc_id]['text'][:200]}...\")\n",
    "\n",
    "print(\"\\nSample Query:\")\n",
    "sample_query_id = list(queries.keys())[0]\n",
    "print(f\"\\nQuery ID: {sample_query_id}\")\n",
    "print(f\"Query: {queries[sample_query_id]}\")\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Number of documents: {len(corpus)}\")\n",
    "print(f\"Number of queries: {len(queries)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#开源的企业知识库评测数据集\n",
    "#query和知识库中召回答案的相似度和排序\n",
    "#调用dify的知识库查询接口\n",
    "#调用灵搭的知识库查询接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "class RetrievalEvaluator:\n",
    "    def __init__(self, queries: List[str], documents: List[str], relevance: Dict):\n",
    "        self.queries = queries\n",
    "        self.documents = documents\n",
    "        self.relevance = relevance\n",
    "        \n",
    "    def evaluate_system(self, retrieval_system, k=10):\n",
    "        \"\"\"评估检索系统\"\"\"\n",
    "        metrics = {\n",
    "            'precision@k': [],\n",
    "            'recall@k': [],\n",
    "            'ndcg@k': []\n",
    "        }\n",
    "        \n",
    "        for query_id, query in enumerate(self.queries):\n",
    "            # 获取系统返回结果\n",
    "            retrieved_docs = retrieval_system.search(query, top_k=k)\n",
    "            \n",
    "            # 计算评估指标\n",
    "            true_relevance = self.get_relevance(query_id)\n",
    "            metrics['precision@k'].append(self.precision_at_k(retrieved_docs, true_relevance, k))\n",
    "            metrics['recall@k'].append(self.recall_at_k(retrieved_docs, true_relevance, k))\n",
    "            metrics['ndcg@k'].append(self.ndcg_at_k(retrieved_docs, true_relevance, k))\n",
    "            \n",
    "        return {metric: np.mean(values) for metric, values in metrics.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluation_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluation_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RetrievalEvaluator\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDifyRetriever\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, api_key):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluation_data'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from evaluation_data import RetrievalEvaluator\n",
    "\n",
    "class DifyRetriever:\n",
    "    def __init__(self, api_key):\n",
    "        self.api_key = api_key\n",
    "        self.api_url = \"https://api.dify.ai/v1/query\"\n",
    "        \n",
    "    def search(self, query: str, top_k: int = 10):\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n",
    "        response = requests.post(\n",
    "            self.api_url,\n",
    "            json={\"query\": query, \"top_k\": top_k},\n",
    "            headers=headers\n",
    "        )\n",
    "        return response.json()[\"documents\"]\n",
    "\n",
    "class EnterpriseRetriever:\n",
    "    def __init__(self, knowledge_base_path):\n",
    "        self.kb_path = knowledge_base_path\n",
    "        # 初始化您的企业知识库检索系统\n",
    "        \n",
    "    def search(self, query: str, top_k: int = 10):\n",
    "        # 实现您的检索逻辑\n",
    "        pass\n",
    "\n",
    "# 使用示例\n",
    "def main():\n",
    "    # 加载测试数据\n",
    "    evaluator = RetrievalEvaluator(\n",
    "        queries=load_test_queries(),\n",
    "        documents=load_test_documents(),\n",
    "        relevance=load_relevance_judgments()\n",
    "    )\n",
    "    \n",
    "    # 初始化检索系统\n",
    "    dify_retriever = DifyRetriever(api_key=\"dataset-vGo3aB5ogicKiIsdWQ5FmBE2\")\n",
    "    enterprise_retriever = EnterpriseRetriever(\"path_to_kb\")\n",
    "    \n",
    "    # 评估两个系统\n",
    "    dify_results = evaluator.evaluate_system(dify_retriever)\n",
    "    enterprise_results = evaluator.evaluate_system(enterprise_retriever)\n",
    "    \n",
    "    # 输出对比结果\n",
    "    print(\"Dify System Results:\", dify_results)\n",
    "    print(\"Enterprise System Results:\", enterprise_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AgentQuery(queryJson):\n",
    "    host = \"https://uat.agentspro.cn/\"\n",
    "    url = host + \"openapi/agent/chat/completions/v1\"\n",
    "\n",
    "    # 请求头通常包含一些如内容类型、认证信息等\n",
    "    headers = {\n",
    "        \"token\":\"ff19bd0cabcf4dcd83898066c1be4eba.ZG5SR1e3CqcXYSMUKIvHPkcVan0SKPMQ\"\n",
    "    }\n",
    "\n",
    "    # 请求体是发送到服务器的数据，以下是一个JSON格式的示例\n",
    "    payload = {\n",
    "        \"agentId\":\"ff19bd0cabcf4dcd83898066c1be4eba\",\n",
    "        \"userChatInput\": queryJson\n",
    "    }  \n",
    "\n",
    "    # 使用requests库发送POST请求\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    # 打印出响应内容\n",
    "    json_data = response.text\n",
    "    data = json.loads(json_data)\n",
    "    content = data['choices'][-1]['content']\n",
    "    return(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
